package deploytest

import (
	"context"
	"crypto"
	"encoding/binary"
	"fmt"
	"github.com/hyperledger-labs/mirbft"
	"github.com/hyperledger-labs/mirbft/pkg/eventlog"
	"github.com/hyperledger-labs/mirbft/pkg/modules"
	"github.com/hyperledger-labs/mirbft/pkg/pb/msgs"
	"github.com/hyperledger-labs/mirbft/pkg/reqstore"
	"github.com/hyperledger-labs/mirbft/pkg/simplewal"
	"github.com/hyperledger-labs/mirbft/pkg/status"
	"io/ioutil"
	"os"
	"path/filepath"
	"runtime/debug"
	"sync"
	"time"

	. "github.com/onsi/ginkgo"
	. "github.com/onsi/gomega"
)

// TestConfig contains the parameters of the deployment to be tested.
type TestConfig struct {

	// Number of replicas in the tested deployment.
	NumReplicas int

	// Number of clients in the tested deployment.
	NumClients int

	// The width of the client watermark window.
	ClientWMWidth int

	// The number of requests each client submits during the execution of the deployment.
	NumRequests int
}

// The Deployment represents a list of replicas interconnected by a simulated network transport.
type Deployment struct {
	Transport    *FakeTransport
	TestReplicas []*TestReplica
}

// NewDeployment returns a Deployment initialized according to the passed configuration.
func NewDeployment(testConfig TestConfig, doneC <-chan struct{}) *Deployment {

	// Create a temporary directory in the OS-default location.
	// This directory will contain the files generated by the deployment.
	tmpDir, err := ioutil.TempDir("", "stress_test.*")
	Expect(err).NotTo(HaveOccurred())

	// Create a simulated network transport to route messages between replicas.
	transport := NewFakeTransport(testConfig.NumReplicas)

	// Create all TestReplicas for this deployment.
	replicas := make([]*TestReplica, testConfig.NumReplicas)
	for i := range replicas {

		Continue here: Clean up configuration parameters
		and separate what belongs to the node and what belongs to the state machine (protocol)

		config := &mirbft.NodeConfig{
			BatchSize:            1,
			SuspectTicks:         4,
			HeartbeatTicks:       2,
			NewEpochTimeoutTicks: 8,
			BufferSize:           5 * 1024 * 1024, // 5 MB
			Logger:               mirbft.ConsoleWarnLogger,
		}

		if testConfig.BatchSize != 0 {
			config.BatchSize = testConfig.BatchSize
		}

		fakeApp := &FakeApp{
			// We make the CommitC excessive, to prevent deadlock
			// in case of bugs this test would otherwise catch.
			CommitC: make(chan *msgs.QEntry, 5*testConfig.MsgCount),
		}

		replicas[i] = &TestReplica{
			ID:                  uint64(i),
			Config:              config,
			InitialNetworkState: networkState,
			TmpDir:              filepath.Join(tmpDir, fmt.Sprintf("node%d", i)),
			App:                 fakeApp,
			FakeTransport:       transport,
			FakeClient: &FakeClient{
				MsgCount: uint64(testConfig.MsgCount),
			},
			ParallelProcess: testConfig.ParallelProcess,
			DoneC:           doneC,
		}
	}

	return &Deployment{
		Transport:    transport,
		TestReplicas: replicas,
	}
}





func clientReq(clientID, reqNo uint64) []byte {
	res := make([]byte, 16)
	binary.BigEndian.PutUint64(res, clientID)
	binary.BigEndian.PutUint64(res[8:], reqNo)
	return res
}

// TestReplica represents one replica (that uses one instance of the mirbft.Node) in the test system.
type TestReplica struct {

	// Closing this channel indicates to the replica to terminate gracefully.
	DoneC <-chan struct{}

	// ID of the replica as seen by the protocol.
	ID uint64

	// Dummy test application the replica is running.
	App *FakeApp

	// Name of the directory where the persisted state of this TestReplica will be stored,
	// along with the logs produced by running the replica.
	TmpDir string

	Config              *mirbft.NodeConfig
	InitialNetworkState *msgs.NetworkState
	FakeTransport       *FakeTransport
	ParallelProcess     bool
}

func (tr *TestReplica) EventLogPath() string {
	return filepath.Join(tr.TmpDir, "eventlog.gz")
}

func (tr *TestReplica) Run(tickInterval time.Duration) (*status.StateMachine, error) {
	defer GinkgoRecover()

	ticker := time.NewTicker(tickInterval)
	defer ticker.Stop()

	reqStorePath := filepath.Join(tr.TmpDir, "reqstore")
	err := os.MkdirAll(reqStorePath, 0700)
	Expect(err).NotTo(HaveOccurred())

	walPath := filepath.Join(tr.TmpDir, "wal")
	err = os.MkdirAll(walPath, 0700)
	Expect(err).NotTo(HaveOccurred())

	file, err := os.Create(tr.EventLogPath())
	Expect(err).NotTo(HaveOccurred())
	defer file.Close()

	interceptor := eventlog.NewRecorder(tr.ID, file)
	defer func() {
		err := interceptor.Stop()
		Expect(err).NotTo(HaveOccurred())
	}()

	wal, err := simplewal.Open(walPath)
	Expect(err).NotTo(HaveOccurred())
	defer wal.Close()

	reqStore, err := reqstore.Open(reqStorePath)
	Expect(err).NotTo(HaveOccurred())
	defer reqStore.Close()

	var wg sync.WaitGroup
	defer wg.Wait()

	node, err := mirbft.NewNode(
		tr.ID,
		tr.Config,
		&modules.Modules{
			Net:          tr.FakeTransport.Link(tr.ID),
			Hasher:       crypto.SHA256,
			RequestStore: reqStore,
			App:          tr.App,
			WAL:          wal,
			Interceptor:  interceptor,
		},
	)
	Expect(err).NotTo(HaveOccurred())

	wg.Add(1)
	go func() {
		defer wg.Done()
		recvC := tr.FakeTransport.RecvC(node.ID)
		for {
			select {
			case sourceMsg := <-recvC:
				node.Step(context.Background(), sourceMsg.Source, sourceMsg.Msg)
			case <-tr.DoneC:
				return
			}
		}
	}()

	expectedProposalCount := tr.FakeClient.MsgCount
	Expect(expectedProposalCount).NotTo(Equal(0))

	wg.Add(1)
	go func() {
		defer wg.Done()
		for {
			select {
			case <-tr.DoneC:
				return
			default:
			}

			for i := uint64(0); i < tr.FakeClient.MsgCount; i++ {
				if err := node.SubmitRequest(context.Background(), 0, i, clientReq(0, i)); err != nil {
					// TODO, failing on err causes flakes in the teardown,
					// so just returning for now, we should address later
					break
				}
			}
		}
	}()

	err = node.ProcessAsNewNode(tr.DoneC, ticker.C, tr.InitialNetworkState, []byte("fake"))
	_ = err // XXX we need to rewire all of this

	return node.Status(context.Background())
}



type NodeStatus struct {
	Status  *status.StateMachine
	ExitErr error
}

func (n *Deployment) Run(tickInterval time.Duration) []*NodeStatus {
	result := make([]*NodeStatus, len(n.TestReplicas))
	var wg sync.WaitGroup

	// Start the Mir nodes
	for i, testReplica := range n.TestReplicas {
		nodeStatus := &NodeStatus{}
		result[i] = nodeStatus
		wg.Add(1)
		go func(i int, testReplica *TestReplica) {
			defer GinkgoRecover()
			defer wg.Done()
			defer func() {
				fmt.Printf("Node %d: shutting down\n", i)
				if r := recover(); r != nil {
					fmt.Printf("  Node %d: received panic %s\n%s\n", i, r, debug.Stack())
					panic(r)
				}
			}()

			fmt.Printf("Node %d: running\n", i)
			status, err := testReplica.Run(tickInterval)
			fmt.Printf("Node %d: exit with exitErr=%v\n", i, err)
			nodeStatus.Status, nodeStatus.ExitErr = status, err
		}(i, testReplica)
	}

	n.Transport.Start()
	defer n.Transport.Stop()

	wg.Wait()

	fmt.Printf("All go routines shut down\n")
	return result
}
